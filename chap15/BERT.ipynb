{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../chap15/NLP_Doc.ipynb\n",
    "%run ../chap15/BERT_dataset.ipynb\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "import scipy\n",
    "import time\n",
    "import re\n",
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NLP_Doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-920e05dafe88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNLP_Doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     def __init__(self, name, dataset, hconfigs, JJ=0, show_maps=False, batch_size=batch_size, \n\u001b[0;32m      3\u001b[0m                  l2_decay=0, l1_decay=0, dump_structure=True, word_vector_dimension=512, window_size=1, negative=5, load=0,num_heads=8):\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vector_dimension\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_vector_dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_unique_words_number\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NLP_Doc' is not defined"
     ]
    }
   ],
   "source": [
    "class transformer(NLP_Doc):\n",
    "    def __init__(self, name, dataset, hconfigs, JJ=0, show_maps=False, batch_size=1, \n",
    "                 l2_decay=0, l1_decay=0, dump_structure=True, word_vector_dimension=512, window_size=1, negative=5, load=0,num_heads=8):\n",
    "        self.word_vector_dimension=word_vector_dimension\n",
    "        self.training_unique_words_number=0\n",
    "        self.training_full_text_number=0\n",
    "        self.new_words_number=0\n",
    "        self.new_full_text_number=0\n",
    "        self.window_size=window_size\n",
    "        self.negative = negative\n",
    "        self.num_heads=num_heads\n",
    "        self.rand_std = 0.03\n",
    "        self.use_adam=1\n",
    "        self.inter_stop=0\n",
    "        self.l2_decay = 0\n",
    "        self.l1_decay = 0\n",
    "        self.dic_training=0\n",
    "        self.is_training=False\n",
    "        self.text_dataset  = transformer_Dataset(['initial'])\n",
    "        self.text_dataset.text_find_number_words(\"text_kor_eng.xlsx\",1)\n",
    "        self.load=load\n",
    "        self.batch_size=batch_size\n",
    "        self.look_ahead_index=0\n",
    "        self.name=name\n",
    "        self.average_normal=0\n",
    "        self.ffnn_size=300\n",
    "        self.warmup_steps=1000\n",
    "        self.init_parameters(hconfigs)\n",
    "        self.root_=np.sqrt(int(self.word_vector_dimension/self.num_heads))\n",
    "        self.n=0\n",
    "        self.label=np.zeros((self.batch_size,2))\n",
    "        self.label[:int(self.batch_size//2),0]=1\n",
    "        self.label[int(self.batch_size//2):,1]=1\n",
    "        #self.load_parameters('dic')\n",
    "        #self.G_input_encoder=0\n",
    "        #self.save_parameters('dic')\n",
    "        \n",
    "        \n",
    "        self.bleu=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6fc8912578b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpm_hiddens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmaking_weight_em\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "def init_parameters(self, hconfigs):\n",
    "    self.hconfigs = hconfigs\n",
    "    self.pm_hiddens = []\n",
    "    pm_hidden=self.alloc_embedding_layer()\n",
    "    self.pm_hiddens.append(pm_hidden)\n",
    "    pm_hidden=self.alloc_po_embedding_layer()\n",
    "    self.pm_hiddens.append(pm_hidden)\n",
    "    pm_hidden=self.alloc_sep_embedding_layer()\n",
    "    self.pm_hiddens.append(pm_hidden)\n",
    "    \n",
    "    prev_shape=(self.text_dataset.most_length,self.word_vector_dimension)\n",
    "    for i in range(self.hconfigs[0][1]['repeat']):\n",
    "        pm_hidden, prev_shape = self.alloc_multi_heads_layer(0)\n",
    "        self.pm_hiddens.append(pm_hidden)\n",
    "    \n",
    "    dic_temp={}\n",
    "    dic_temp['w'], dic_temp['b']=transformer.making_weight(self,self.word_vector_dimension,len(self.text_dataset.original_id_to_word))\n",
    " \n",
    "    dic_temp['w_cla'], dic_temp['b_cla']=transformer.making_weight(self,self.word_vector_dimension,2)\n",
    "    self.pm_hiddens.append(dic_temp)\n",
    "        \n",
    "transformer.init_parameters = init_parameters\n",
    "\n",
    "def making_weight_em(self, row, column):\n",
    "    np.random.seed(int(time.time()*10000000))\n",
    "    weight= np.random.normal(self.average_normal, self.rand_std, [row, column], dtype = 'float32')\n",
    "    time.sleep(0.001)\n",
    "    return weight\n",
    "\n",
    "def making_weight(self, row, column): #1d\n",
    "    np.random.seed(int(time.time()*10000000))\n",
    "    weight= np.random.normal(self.average_normal, self.rand_std, [row, column], dtype = 'float32')\n",
    "    time.sleep(0.001)\n",
    "    bias = np.zeros([column], dtype = 'float32')\n",
    "    return weight, bias\n",
    "\n",
    "transformer.making_weight_em=making_weight_em\n",
    "transformer.making_weight=making_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alloc_multi_heads_layer(self,decoder=0):\n",
    "    temp_list=[]\n",
    "    \n",
    "    if self.word_vector_dimension%self.num_heads !=0:\n",
    "        raise NameError('self.word_vector_dimension%self.num_heads')\n",
    "    \n",
    "    input_cnt = np_cpu.prod(self.word_vector_dimension)\n",
    "    output_cnt = int(self.word_vector_dimension/self.num_heads)\n",
    "    print(\"self.num_heads:\",self.num_heads,output_cnt)\n",
    "    for i in range(self.num_heads):\n",
    "        temp_dic={}\n",
    "        temp_dic[str(i)+'_q'],temp_dic[str(i)+'_qb']=transformer.making_weight(self,input_cnt, output_cnt)\n",
    "        temp_dic[str(i)+'_k'],temp_dic[str(i)+'_kb']=transformer.making_weight(self,input_cnt, output_cnt)\n",
    "        temp_dic[str(i)+'_v'],temp_dic[str(i)+'_vb']=transformer.making_weight(self,input_cnt, output_cnt)\n",
    "\n",
    "        if decoder ==1:\n",
    "            temp_dic[str(i)+'_q2'],temp_dic[str(i)+'_q2b']=transformer.making_weight(self,input_cnt, output_cnt)\n",
    "            temp_dic[str(i)+'_k2'],temp_dic[str(i)+'_k2b']=transformer.making_weight(self,input_cnt, output_cnt)\n",
    "            temp_dic[str(i)+'_v2'],temp_dic[str(i)+'_v2b']=transformer.making_weight(self,input_cnt, output_cnt)\n",
    "        temp_list.append(temp_dic)\n",
    "    \n",
    "    temp_dic={}\n",
    "    temp_dic['0_w'],temp_dic['0_wb']=transformer.making_weight(self,input_cnt, input_cnt)\n",
    "    temp_dic['1_w'],temp_dic['1_wb']=transformer.making_weight(self,input_cnt,self.ffnn_size)\n",
    "    temp_dic['2_w'],temp_dic['2_wb']=transformer.making_weight(self,self.ffnn_size,input_cnt)\n",
    "    temp_dic['2_batch'],_=transformer.alloc_batch_normal_layer(self,input_cnt,None)\n",
    "    temp_dic['0_batch'],_=transformer.alloc_batch_normal_layer(self,input_cnt,None)\n",
    "    if decoder == 1:\n",
    "        temp_dic['3_w'],temp_dic['3_wb']=transformer.making_weight(self,input_cnt, input_cnt)\n",
    "        temp_dic['1_batch'],_=transformer.alloc_batch_normal_layer (self,input_cnt,None)\n",
    "    temp_list.append(temp_dic)\n",
    "    \n",
    "    return temp_list, (self.batch_size,self.text_dataset.most_length,self.word_vector_dimension)\n",
    "\n",
    "transformer.alloc_multi_heads_layer=alloc_multi_heads_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0423758346dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model {} train ended in {} secs:'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtm_total\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "def train(self,epoch_count, learning_rate, learning_decrease,restore, name):\n",
    "    print(\"the number of words and the number of unique words\")\n",
    "    print(len(self.text_dataset.original_text),self.text_dataset.original_unique_words)\n",
    "    print(self.text_dataset.original_id_to_word)\n",
    "    print(self.text_dataset.most_length)\n",
    "    if self.load == 1:\n",
    "        self.load_parameters(\"dic\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    batch_count = int(self.text_dataset.train_count / self.batch_size)\n",
    "    \n",
    "    time1 = time2 = int(time.time())\n",
    "    for epoch in range(epoch_count):\n",
    "        costs = []\n",
    "        bleu = []\n",
    "        bleu_ = []\n",
    "        self.epoch = epoch+1\n",
    "        print(\"self.epoch\",self.epoch)\n",
    "        #self.learning_rate = self.word_vector_dimension**(-0.5)*np.min(np.array([self.epoch**(-0.5),self.epoch*self.warmup_steps**(-1.5)]))\n",
    "        #print(self.learning_rate)\n",
    "        self.learning_rate=learning_rate*learning_decrease\n",
    "        self.text_dataset.shuffle_train_data(self.batch_size*batch_count)\n",
    "        \n",
    "        for n in range(batch_count):\n",
    "            self.n=n\n",
    "            \n",
    "            trX = self.text_dataset.get_train_data(self.batch_size, n)\n",
    "            cost= self.train_step(trX)\n",
    "            #bleu.append(bleu_item)\n",
    "            print(\"loss: \",np.average(cost))\n",
    "            self.test()\n",
    "            #if self.epoch%20==0 and self.epoch!=0:\n",
    "                #bleu_.append(self.bleu_)\n",
    "            #print(\"악\",trX)\n",
    "        #print(\"bleu: \",np.average(np.array(bleu)))\n",
    "        #if self.epoch%20==0 and  self.epoch!=0:\n",
    "            #print(\"bleu_real: \",np.average(np.array(bleu_)))\n",
    "        if epoch % 5 ==0and epoch !=0:\n",
    "            save_parameters(self,'dic')\n",
    "        \n",
    "    tm_total = int(time.time()) - time1\n",
    "    print('Model {} train ended in {} secs:'.format(self.name, tm_total))\n",
    "\n",
    "transformer.train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(self,char):    #for word vector\n",
    "\n",
    "    time1 = int(time.time())\n",
    "\n",
    "    with open(\"hyper_\"+self.name+\".csv\",'w',newline='') as aa:\n",
    "        \n",
    "        print(self.hconfigs, file=aa)\n",
    "        print(self.word_vector_dimension, file=aa)\n",
    "        print(self.training_unique_words_number,file=aa)\n",
    "        print(self.training_full_text_number,file=aa)\n",
    "        print(self.new_words_number,file=aa)\n",
    "        print(self.new_full_text_number,file=aa)\n",
    "        print(self.batch_size,file=aa)\n",
    "        print(self.ffnn_size,file=aa)\n",
    "        print(self.pm_hiddens[0]['dic'].shape,file=aa)\n",
    "        \n",
    "        #print(self.pm_hiddens)\n",
    "    with open(\"dic_original_\"+self.name+\"_.csv\",'w',newline='') as aa:\n",
    "        for i in range(len(self.text_dataset.original_id_to_word)):\n",
    "            print(self.text_dataset.original_id_to_word[i], file=aa)\n",
    "            \n",
    "    self.save_ass(\"word_vector_original_\",self.pm_hiddens[0]['dic'])\n",
    "    \n",
    "    self.save_ass(\"word_vector_pos_\",self.pm_hiddens[1]['dic'])\n",
    "    \n",
    "    self.save_ass(\"word_vector_sep_\",self.pm_hiddens[2]['dic'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    for j in range(self.hconfigs[0][1]['repeat']):\n",
    "        pm=self.pm_hiddens[j+3]\n",
    "        for i in range(self.num_heads):\n",
    "            self.save_ass(\"encoder\"+str(j)+str(i)+\"below_q\",pm[i][str(i)+'_q'])\n",
    "            self.save_ass(\"encoder\"+str(j)+str(i)+\"below_k\",pm[i][str(i)+'_k'])\n",
    "            self.save_ass(\"encoder\"+str(j)+str(i)+\"below_v\",pm[i][str(i)+'_v'])\n",
    "            self.save_ass_1(\"encoder\"+str(j)+str(i)+\"below_qb\",pm[i][str(i)+'_qb'])\n",
    "            self.save_ass_1(\"encoder\"+str(j)+str(i)+\"below_kb\",pm[i][str(i)+'_kb'])\n",
    "            self.save_ass_1(\"encoder\"+str(j)+str(i)+\"below_vb\",pm[i][str(i)+'_vb'])\n",
    "\n",
    "        self.save_ass(\"encoder\"+str(j)+\"upper_0_w\",pm[-1]['0_w'])\n",
    "        self.save_ass(\"encoder\"+str(j)+\"upper_1_w\",pm[-1]['1_w'])\n",
    "        self.save_ass(\"encoder\"+str(j)+\"upper_2_w\",pm[-1]['2_w'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_0_wb\",pm[-1]['0_wb'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_1_wb\",pm[-1]['1_wb'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_2_wb\",pm[-1]['2_wb'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_0_batch_mavg\",pm[-1]['0_batch']['mavg'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_0_batch_mvar\",pm[-1]['0_batch']['mvar'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_2_batch_mavg\",pm[-1]['2_batch']['mavg'])\n",
    "        self.save_ass_1(\"encoder\"+str(j)+\"upper_2_batch_mvar\",pm[-1]['2_batch']['mvar'])\n",
    "\n",
    "    \n",
    " \n",
    "            \n",
    "  \n",
    "    self.save_ass(\"output_\",self.pm_hiddens[-1]['w'])\n",
    "    self.save_ass_1(\"output_b\",self.pm_hiddens[-1]['b'])\n",
    "   \n",
    "    \n",
    "            \n",
    "            \n",
    "def save_ass(self,name,pm):\n",
    "    with open(name+\"_.csv\",'w',newline='') as aa:\n",
    "        temp_list=list()\n",
    "        for i in range(pm.shape[0]):\n",
    "            temp_list=list()\n",
    "            re_before=pm[i,:].tolist()\n",
    "            for j in range(len(re_before)):\n",
    "                re_before[j]=re.sub('\\\\n','',str(re_before[j]))\n",
    "                temp_list.append(re_before[j])\n",
    "            print(temp_list, file=aa)\n",
    "\n",
    "def save_ass_1(self,name,pm):\n",
    "    with open(name+\"_.csv\",'w',newline='') as aa:\n",
    "        re_before=pm.tolist()\n",
    "        for j in range(len(re_before)):\n",
    "            re_before[j]=re.sub('\\\\n','',str(re_before[j]))\n",
    " \n",
    "        print(re_before, file=aa)\n",
    "            \n",
    "def load_parameters(self,char): #for word vector\n",
    "    self.load_ass(\"word_vector_original_\",self.pm_hiddens[0]['dic'])\n",
    "    self.load_ass(\"word_vector_po_\",self.pm_hiddens[1]['dic'])\n",
    "    self.load_ass(\"word_vector_sep_\",self.pm_hiddens[2]['dic'])\n",
    "    \n",
    "    for j in range(self.hconfigs[0][1]['repeat']):\n",
    "        pm=self.pm_hiddens[j+1]\n",
    "        for i in range(self.num_heads):\n",
    "            self.load_ass(\"encoder\"+str(j)+str(i)+\"below_q\",pm[i][str(i)+'_q'])\n",
    "            self.load_ass(\"encoder\"+str(j)+str(i)+\"below_k\",pm[i][str(i)+'_k'])\n",
    "            self.load_ass(\"encoder\"+str(j)+str(i)+\"below_v\",pm[i][str(i)+'_v'])\n",
    "            self.load_ass_1(\"encoder\"+str(j)+str(i)+\"below_qb\",pm[i][str(i)+'_qb'])\n",
    "            self.load_ass_1(\"encoder\"+str(j)+str(i)+\"below_kb\",pm[i][str(i)+'_kb'])\n",
    "            self.load_ass_1(\"encoder\"+str(j)+str(i)+\"below_vb\",pm[i][str(i)+'_vb'])\n",
    "\n",
    "        self.load_ass(\"encoder\"+str(j)+\"upper_0_w\",pm[-1]['0_w'])\n",
    "        self.load_ass(\"encoder\"+str(j)+\"upper_1_w\",pm[-1]['1_w'])\n",
    "        self.load_ass(\"encoder\"+str(j)+\"upper_2_w\",pm[-1]['2_w'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_0_wb\",pm[-1]['0_wb'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_1_wb\",pm[-1]['1_wb'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_2_wb\",pm[-1]['2_wb'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_0_batch_mavg\",pm[-1]['0_batch']['mavg'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_0_batch_mvar\",pm[-1]['0_batch']['mvar'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_2_batch_mavg\",pm[-1]['2_batch']['mavg'])\n",
    "        self.load_ass_1(\"encoder\"+str(j)+\"upper_2_batch_mvar\",pm[-1]['2_batch']['mvar'])\n",
    "    \n",
    " \n",
    "            \n",
    "    self.load_ass(\"output_\",self.pm_hiddens[-1]['w'])\n",
    "    self.load_ass_1(\"output_b\",self.pm_hiddens[-1]['b'])\n",
    "\n",
    "\n",
    "def load_ass(self,name,pm):\n",
    "    print(name)\n",
    "    with open(name+\"_.csv\",'r',newline='') as aa:      \n",
    "        for i in range(pm.shape[0]):\n",
    "            temp_line=aa.readline().split(',')\n",
    "        \n",
    "            for j in range(len(temp_line)):\n",
    "                if j ==len(temp_line)-1:\n",
    "                    try:\n",
    "                        pm[i,j]=float(temp_line[j][2:-3])\n",
    "                    except:\n",
    "                        print(temp_line)\n",
    "                else:\n",
    "                    try:\n",
    "                        pm[i,j]=float(temp_line[j][2:-1])\n",
    "                    except:\n",
    "                        print(temp_line)\n",
    "\n",
    "def load_ass_1(self,name,pm):\n",
    "    print(name)\n",
    "    with open(name+\"_.csv\",'r',newline='') as aa:      \n",
    "        temp_line=aa.readline().split(',')\n",
    "\n",
    "        for j in range(len(temp_line)):\n",
    "            if j ==len(temp_line)-1:\n",
    "                pm[j]=float(temp_line[j][2:-3])\n",
    "            else:\n",
    "                pm[j]=float(temp_line[j][2:-1])\n",
    "    \n",
    "transformer.load_ass_1=load_ass_1\n",
    "transformer.load_ass=load_ass\n",
    "transformer.save_ass_1=save_ass_1\n",
    "transformer.save_ass=save_ass\n",
    "transformer.load_parameters=load_parameters\n",
    "transformer.save_parameters=save_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def train_step(self, x):\n",
    "    loss_temp=list()\n",
    "    y=0\n",
    "    self.first_time=1\n",
    "    self.is_training = True\n",
    "    loss, aux_nn = self.forward_neuralnet(x,y)\n",
    "    loss_temp.append(loss)\n",
    "    G_loss = 1.0\n",
    "    self.backprop_neuralnet(G_loss, aux_nn)\n",
    "    self.is_training = False\n",
    "    \n",
    "    \n",
    "    return np.mean(np.array(loss_temp))\n",
    "\n",
    "transformer.train_step=train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_neuralnet(self, x,y):\n",
    "    hidden = x\n",
    "    #print(x[0])\n",
    "    aux_layers = [] #embedding, encoder, decoder 1ch/  embedding [interlalyer] output\n",
    "    hidden_1, aux = self.forward_embedding_layer(hidden, None, self.pm_hiddens[0])\n",
    "    \n",
    "    aux_layers.append(aux)\n",
    "    hidden_2, aux = self.forward_po_embedding_layer(hidden, None, self.pm_hiddens[1])\n",
    "    aux_layers.append(aux)\n",
    "    hidden_3, aux = self.forward_sep_embedding_layer(hidden, None, self.pm_hiddens[2])\n",
    "    #print(self.pm_hiddens[0])\n",
    "    aux_layers.append(aux)\n",
    "    #print(hidden_1.shape,hidden_2.shape,hidden_3.shape)\n",
    "    hidden =hidden_1+hidden_2+hidden_3\n",
    "    hidden=self.forward_mask_layer(hidden)\n",
    "    #for i in range(hidden.shape[0]):\n",
    "    ##    for j in range(hidden.shape[1]):\n",
    "    #        print(hidden[i,j])\n",
    "    aux_layers_temp=[]\n",
    "    \n",
    "    for i in range(self.hconfigs[0][1]['repeat']):\n",
    "        hidden, aux = self.forward_multi_heads_layer(hidden, self.pm_hiddens[i+3],0)\n",
    "        aux_layers_temp.append(aux)\n",
    "\n",
    "    aux_layers.append(aux_layers_temp)\n",
    "    aux_layers.append(hidden)\n",
    "    output=np.matmul(hidden[:,self.number,:],self.pm_hiddens[-1]['w'])+self.pm_hiddens[-1]['b']\n",
    "    self.output_cla=np.matmul(hidden[:,0,:],self.pm_hiddens[-1]['w_cla'])+self.pm_hiddens[-1]['b_cla']\n",
    "    #print(self.output_cla.shape)\n",
    "    #print(output)\n",
    "    \n",
    "    self.final_cla=softmax_cross_entropy_with_logits(self.label , self.output_cla)\n",
    "    #print(self.G_input_cla.shape)\n",
    "    y=x[:,self.number]\n",
    "          \n",
    "    result__=np.zeros(np.array(y).shape)\n",
    "    label=self.text_dataset.training_original_one_hot_word[y]\n",
    "    #print(label.shape)\n",
    "    for i in range(y.shape[0]):\n",
    "        result__[i]=softmax_cross_entropy_with_logits(label[i]\n",
    "                                                              , output[i,:,:])\n",
    "    #print(result__[i])\n",
    "    return np.mean(result__), [aux_layers, output, result__,self.text_dataset.training_original_one_hot_word[y]]\n",
    "\n",
    "transformer.forward_neuralnet=forward_neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d1a83bab831b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mG_hidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop_neuralnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbackprop_neuralnet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "def backprop_neuralnet(self, G_loss, aux):\n",
    "    #print(G_loss)\n",
    "    aux_layers, output, result__, y= aux\n",
    "    \n",
    "    g_loss_entropy = 1.0 / np_cpu.prod(result__.shape)\n",
    "    g_entropy_output=np.zeros(y.shape)\n",
    "    for i in range(y.shape[0]):\n",
    "        \n",
    "        g_entropy_output[i] = softmax_cross_entropy_with_logits_derv(y[i,:,:], output[i,:,:])\n",
    "    G_entropy = g_loss_entropy * G_loss\n",
    "    G_affine = g_entropy_output * G_entropy\n",
    "    G_bias=np.sum(G_affine, axis = (0,1))\n",
    "    g_affine_input = self.pm_hiddens[-1]['w'].transpose()\n",
    "    G_weight_=np.zeros_like(self.pm_hiddens[-1]['w'])\n",
    "    G_hidden=np.zeros_like(aux_layers[-1])\n",
    "    \n",
    "    #------------------------\n",
    "    \n",
    "    #------------------------\n",
    "    #print(G_hidden)\n",
    "    #print(G_affine.shape)\n",
    "    #print(G_weight_.shape)\n",
    "    #print(aux_layers[-1].shape)\n",
    "    #print(aux_layers[-1][0,self.number,:].transpose().shape)\n",
    "    #print(np.matmul(aux_layers[-1][i,self.number,:].transpose(),G_affine[i]).shape)\n",
    "    for i in range(G_affine.shape[0]):\n",
    "        if i ==0:\n",
    "            G_weight_=np.matmul(aux_layers[-1][i,self.number,:].transpose(),G_affine[i])\n",
    "        else:\n",
    "            G_weight_+=np.matmul(aux_layers[-1][i,self.number,:].transpose(),G_affine[i])\n",
    "\n",
    "    self.update_param(self.pm_hiddens[-1], 'w', G_weight_)\n",
    "    self.update_param(self.pm_hiddens[-1], 'b', G_bias)\n",
    "    G_hidden[:,self.number,:] = np.matmul(G_affine, g_affine_input)\n",
    "    #print(G_hidden)\n",
    "    #---------------------\n",
    "    g_loss_entropy = 1.0 / np_cpu.prod(self.final_cla.shape)\n",
    "    g_entropy_output=np.zeros(self.label.shape)\n",
    "    #print(self.label.shape, self.output_cla.shape)\n",
    "    \n",
    "    g_entropy_output = softmax_cross_entropy_with_logits_derv(self.label, self.output_cla)\n",
    "    G_entropy = g_loss_entropy * G_loss\n",
    "    G_affine = g_entropy_output * G_entropy\n",
    "    #print(aux_layers[-1][:,0,:].transpose().shape,G_affine.shape)\n",
    "    G_bias=np.sum(G_affine, axis = 0)\n",
    "    g_affine_input = self.pm_hiddens[-1]['w_cla'].transpose()\n",
    "    G_weight_=np.zeros_like(self.pm_hiddens[-1]['w_cla'])\n",
    "    \n",
    "    G_weight_=np.matmul(aux_layers[-1][:,0,:].transpose(),G_affine)\n",
    "        \n",
    "\n",
    "    self.update_param(self.pm_hiddens[-1], 'w_cla', G_weight_)\n",
    "    self.update_param(self.pm_hiddens[-1], 'b_cla', G_bias)\n",
    "    G_hidden[:,0,:] = np.matmul(G_affine, g_affine_input)\n",
    "    #---------------\n",
    "    aux_layers_temp=aux_layers[-2]\n",
    "    \n",
    "    for i in reversed(range(self.hconfigs[0][1]['repeat'])):\n",
    "        pm, aux = self.pm_hiddens[i+3], aux_layers_temp[i]\n",
    "        G_hidden = self.backprop_multi_heads_layer(G_hidden, pm,aux,0)\n",
    "    #print(G_hidden)\n",
    "    G_hidden=self.backprop_mask_layer(G_hidden)\n",
    "    #print(G_hidden)\n",
    "    aux_layers_temp=aux_layers[2]\n",
    "    _ = self.backprop_sep_embedding_layer(G_hidden, None, self.pm_hiddens[2],aux_layers_temp)\n",
    "    aux_layers_temp=aux_layers[1]\n",
    "    _ = self.backprop_po_embedding_layer(G_hidden, None, self.pm_hiddens[1],aux_layers_temp)\n",
    "    aux_layers_temp=aux_layers[0]\n",
    "    _ = self.backprop_embedding_layer(G_hidden, None, self.pm_hiddens[0],aux_layers_temp)\n",
    "\n",
    "    return G_hidden\n",
    "\n",
    "transformer.backprop_neuralnet=backprop_neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-357a2ad0c91e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfinal_attention_conca\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maux_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_multi_heads_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforward_multi_heads_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "def forward_multi_heads_layer(self, x, pm,decoder=0):\n",
    "    \n",
    "    aux_layers=[]\n",
    "    for i in range(self.num_heads):\n",
    "        q_matrix=np.matmul(x,pm[i][str(i)+'_q'])+pm[i][str(i)+'_qb']\n",
    "        k_matrix=np.matmul(x,pm[i][str(i)+'_k'])+pm[i][str(i)+'_kb']\n",
    "        v_matrix=np.matmul(x,pm[i][str(i)+'_v'])+pm[i][str(i)+'_vb']\n",
    "        #print(\"!\",q_matrix.shape, x.shape)\n",
    "        list_before_conca=list()\n",
    "        after_softmax_temp=[]\n",
    "        for j in range(self.batch_size):\n",
    "            before_softmax=np.matmul(q_matrix[j,:,:],k_matrix[j,:,:].transpose())\\\n",
    "            /np.sqrt(int(self.word_vector_dimension/self.num_heads))\n",
    "            #print(q_matrix[j,:,:].shape,k_matrix[j,:,:].shape)\n",
    "            if decoder ==1:\n",
    "                \n",
    "                pass\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                for k in range(len(self.input_mask[0])-self.text_dataset.most_length):\n",
    "                    if self.input_mask[j,k]==1:\n",
    "                        before_softmax[:,k:self.text_dataset.most_length]=(-1e9)\n",
    "        \n",
    "                        break\n",
    "                for k in range(len(self.input_mask[0])-self.text_dataset.most_length):\n",
    "                    if self.input_mask[j,k+self.text_dataset.most_length]==1:\n",
    "                        before_softmax[:,k+self.text_dataset.most_length:]=(-1e9)\n",
    "        \n",
    "                        break\n",
    "\n",
    "            #print(before_softmax)\n",
    "            after_softmax = softmax(before_softmax)\n",
    "            temp_result=np.matmul(after_softmax,v_matrix[j,:,:])\n",
    "            list_before_conca.append(temp_result)\n",
    "            after_softmax_temp.append(after_softmax)\n",
    "\n",
    "        aux_layers.append([x, q_matrix,k_matrix,before_softmax,\\\n",
    "                                      np.array(after_softmax_temp),v_matrix])\n",
    "        list_before_conca=np.array(list_before_conca)\n",
    "        \n",
    "        if i == 0:\n",
    "            final_attention_conca=list_before_conca\n",
    "        else:\n",
    "            final_attention_conca = np.concatenate((final_attention_conca,list_before_conca),axis=2)\n",
    "    \n",
    "\n",
    "    aux_layers.append(final_attention_conca)#1\n",
    "\n",
    "    final_attention_conca=np.matmul(final_attention_conca,pm[-1]['0_w'])+pm[-1]['0_wb']\n",
    "  \n",
    "    final_attention_conca_temp,temp=self.forward_batch_normal_layer(final_attention_conca+x,None,pm[-1]['0_batch']) #+x\n",
    "    aux_layers.append(temp) #6#3\n",
    "    aux_layers.append(final_attention_conca) #2\n",
    "    #atteintion_last\n",
    "    \n",
    "    \n",
    "    \n",
    "    final_attention_conca=relu(np.matmul(final_attention_conca_temp\\\n",
    "                    ,pm[-1]['1_w'])+pm[-1]['1_wb'])\n",
    "    aux_layers.append(final_attention_conca) #-3\n",
    "    #print(final_attention_conca_temp.shape,pm[-1]['1_w'].shape)\n",
    "    final_attention_conca=np.matmul(final_attention_conca\\\n",
    "                    ,pm[-1]['2_w'])+pm[-1]['2_wb']\n",
    "    aux_layers.append(final_attention_conca) #-2\n",
    "    final_attention_conca,temp = self.forward_batch_normal_layer(final_attention_conca+final_attention_conca_temp,None,pm[-1]['2_batch']) #+final_attention_conca_temp\n",
    "    aux_layers.append(temp) #-1\n",
    "\n",
    "    return final_attention_conca,aux_layers\n",
    "\n",
    "transformer.forward_multi_heads_layer=forward_multi_heads_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-2-ce694683a438>, line 301)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-ce694683a438>\"\u001b[1;36m, line \u001b[1;32m301\u001b[0m\n\u001b[1;33m    transformer.backprop_multi_heads_layer=backprop_multi_heads_layer\u001b[0m\n\u001b[1;37m                                                                     \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "def backprop_multi_heads_layer(self, G_input, pm, aux,decoder):\n",
    "\n",
    "    if pm is None: return G_y\n",
    "    \n",
    "    \n",
    "    G_input_shape=G_input.shape\n",
    "    G_input=G_input.reshape(-1)\n",
    "    G_input=np.where(G_input<1e2,G_input,1e2)\n",
    "    G_input=np.where(G_input<-1e2,-1e2,G_input)\n",
    "    G_input=np.where(np.abs(G_input)<1e-5,0,G_input)\n",
    "    G_input=G_input.reshape(G_input_shape)\n",
    "    \n",
    "    G_input = self.backprop_batch_normal_layer(G_input,None,pm[-1]['2_batch'],aux[-1])\n",
    "    #print(G_input)\n",
    "    new_G_input=np.copy(G_input)                          \n",
    "    g_affine_input = pm[-1]['2_w'].transpose()\n",
    "    \n",
    "    for i in range(G_input.shape[0]):\n",
    "        if i ==0:\n",
    "            G_weight_=np.matmul(aux[-3][i].transpose(),G_input[i])\n",
    "        else:\n",
    "            G_weight_+=np.matmul(aux[-3][i].transpose(),G_input[i])\n",
    "    #print(g_affine_input.shape,G_input.shape,aux[-3][0].transpose().shape,G_weight_.shape)\n",
    "    G_bias=np.sum(G_input, axis = (0,1))\n",
    "    self.update_param(pm[-1], '2_wb', G_bias)\n",
    "    self.update_param(pm[-1], '2_w', G_weight_)\n",
    "    \n",
    "    G_input = np.matmul(G_input, g_affine_input)\n",
    "    G_input = G_input*relu_derv(G_input)\n",
    "\n",
    "    g_affine_input = pm[-1]['1_w'].transpose()\n",
    "    #print(G_input.shape, aux[-4].shape)\n",
    "    for i in range(G_input.shape[0]):\n",
    "        if i ==0:\n",
    "            G_weight_=np.matmul(aux[-4][i].transpose(),G_input[i])\n",
    "        else:\n",
    "            G_weight_+=np.matmul(aux[-4][i].transpose(),G_input[i])\n",
    "    G_bias=np.sum(G_input, axis = (0,1))\n",
    "    self.update_param(pm[-1], '1_wb', G_bias)\n",
    "    self.update_param(pm[-1], '1_w', G_weight_)\n",
    "    G_input_second = np.matmul(G_input, g_affine_input)+new_G_input\n",
    "\n",
    "    #print(G_input_second)\n",
    "    \n",
    "    #print(G_input_second)\n",
    "    G_input_second_shape=G_input_second.shape\n",
    "    G_input_second=G_input_second.reshape(-1)\n",
    "    \n",
    "    G_input_second=np.where(G_input_second<1e2,G_input_second,1e2)\n",
    "    G_input_second=np.where(G_input_second<-1e2,-1e2,G_input_second)\n",
    "    G_input_second=np.where(np.abs(G_input_second)<1e-5,0,G_input_second)\n",
    "    \n",
    "    G_input_second=G_input_second.reshape(G_input_second_shape)\n",
    "    G_input_second = self.backprop_batch_normal_layer(G_input_second,None,pm[-1]['0_batch'],aux[self.num_heads+1])\n",
    "    \n",
    "    new_G_input=np.copy(G_input_second)\n",
    "    \n",
    "    \n",
    "    g_affine_input = pm[-1]['0_w'].transpose()\n",
    "\n",
    "    for i in range( G_input_second.shape[0]):\n",
    "        if i ==0:\n",
    "            G_weight_=np.matmul(aux[self.num_heads][i].transpose(),G_input_second[i])\n",
    "        else:\n",
    "            G_weight_+=np.matmul(aux[self.num_heads][i].transpose(),G_input_second[i])\n",
    "            \n",
    "    G_bias=np.sum(G_input_second, axis = (0,1))\n",
    "    self.update_param(pm[-1], '0_wb', G_bias)\n",
    "    self.update_param(pm[-1], '0_w', G_weight_)\n",
    "    #print(G_bias)\n",
    "    \n",
    "    G_input_final=np.zeros_like(G_input_second)\n",
    "    G_input_ = np.matmul(G_input_second, g_affine_input)\n",
    "    #print(G_input_)\n",
    "    for i in range(self.num_heads):\n",
    "\n",
    "        G_input_temp=G_input_[:,:,(G_input_.shape[-1]/self.num_heads)*i:(G_input_.shape[-1]/self.num_heads)*(i+1)]\n",
    "        aux_temp_num=aux[i]\n",
    "        for j in range(self.batch_size):\n",
    "            if j == 0:\n",
    "                g_affine_weight = aux_temp_num[-2][j,:,:].transpose()\n",
    "                g_affine_input = aux_temp_num[-1][j,:,:].transpose()\n",
    "                G_v = np.matmul(g_affine_weight, G_input_temp[j,:,:])\n",
    "                G_input = np.matmul(G_input_temp[j,:,:], g_affine_input)\n",
    "                G_input = G_input*softmax_derv(G_input)\n",
    "                #print(G_input)\n",
    "                #print(G_input)\n",
    "                if decoder ==1:\n",
    "\n",
    "                    pass\n",
    "                    \n",
    "                else:\n",
    "                    for k in range(len(self.input_mask[0])-self.text_dataset.most_length):\n",
    "                        if self.input_mask[j,k]==1:\n",
    "                            G_input[:,k:self.text_dataset.most_length]=0\n",
    "\n",
    "                            break\n",
    "                    for k in range(len(self.input_mask[0])-self.text_dataset.most_length):\n",
    "                        if self.input_mask[j,k+self.text_dataset.most_length]==1:\n",
    "                            G_input[:,k+self.text_dataset.most_length:]=0\n",
    "\n",
    "                            break\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                G_input=G_input/self.root_\n",
    "                g_affine_weight = aux_temp_num[1][j,:,:].transpose()\n",
    "                g_affine_input = aux_temp_num[2][j,:,:]\n",
    "                G_k = np.matmul(g_affine_weight, G_input).transpose()\n",
    "                G_q = np.matmul(G_input, g_affine_input)\n",
    "         \n",
    "                G_v = G_v[np.newaxis,:,:]\n",
    "                G_k = G_k[np.newaxis,:,:]\n",
    "                G_q = G_q[np.newaxis,:,:]\n",
    "    \n",
    "            else:\n",
    "                g_affine_weight = aux_temp_num[-2][j,:,:].transpose()\n",
    "                g_affine_input = aux_temp_num[-1][j,:,:].transpose()\n",
    "                G_v_temp=np.matmul(g_affine_weight, G_input_temp[j,:,:])\n",
    "                G_v_temp=G_v_temp[np.newaxis,:,:]\n",
    "                G_input = np.matmul(G_input_temp[j,:,:], g_affine_input)\n",
    "                G_input = G_input*softmax_derv(G_input)\n",
    "                if decoder ==1:\n",
    "\n",
    "                    for k in range(len(self.input_mask_target[0])):\n",
    "                        G_input[k,k+1:]=0\n",
    "                    #print(G_input)\n",
    "                    \n",
    "                    for k in range(len(self.input_mask_target[0])):\n",
    "                        if self.input_mask_target[j,k]==0:\n",
    "                            G_input[:,k:]=0\n",
    "                            break\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    \n",
    "                    for k in range(len(self.input_mask[0])):\n",
    "                        if self.input_mask[j,k]==1:\n",
    "                            G_input[:,k:]=0\n",
    "                            break\n",
    "                    \n",
    "                #print(G_input)\n",
    "                G_input=G_input/self.root_\n",
    "                \n",
    "                g_affine_weight = aux_temp_num[1][j,:,:].transpose()\n",
    "                g_affine_input = aux_temp_num[2][j,:,:]\n",
    "                G_k_temp = np.matmul(g_affine_weight, G_input).transpose()\n",
    "                G_k_temp=G_k_temp[np.newaxis,:,:]\n",
    "                G_q_temp = np.matmul(G_input, g_affine_input)\n",
    "                G_q_temp=G_q_temp[np.newaxis,:,:]\n",
    "                G_q = np.concatenate((G_q, G_q_temp),axis=0)\n",
    "                G_v = np.concatenate((G_v, G_v_temp),axis=0)\n",
    "                G_k = np.concatenate((G_k, G_k_temp),axis=0)\n",
    "     \n",
    "    \n",
    "        \n",
    "        for j in range(self.batch_size):\n",
    "            g_affine_weight = aux_temp_num[0][j].transpose()\n",
    "   \n",
    "            if j==0:\n",
    "                G_weight_8 = np.matmul(g_affine_weight, G_k[j,:,:])\n",
    "            else:\n",
    "                G_weight_8 += np.matmul(g_affine_weight, G_k[j,:,:])\n",
    "\n",
    "            if j==0:\n",
    "                G_weight_9 = np.matmul(g_affine_weight, G_v[j,:,:])\n",
    "            else:\n",
    "                G_weight_9 += np.matmul(g_affine_weight, G_v[j,:,:])\n",
    " \n",
    "            if j==0:\n",
    "                G_weight_10= np.matmul(g_affine_weight, G_q[j,:,:])\n",
    "            else:\n",
    "                G_weight_10+= np.matmul(g_affine_weight, G_q[j,:,:])\n",
    "   \n",
    "        #print(G_q)\n",
    "    \n",
    "        \n",
    "        G_bias=np.sum(G_q, axis = (0,1))\n",
    "        #print(G_bias)\n",
    "        self.update_param(pm[i], str(i)+'_qb', G_bias)\n",
    "        \n",
    "        G_bias=np.sum(G_k, axis = (0,1))\n",
    "        self.update_param(pm[i], str(i)+'_kb', G_bias)\n",
    "        G_bias=np.sum(G_v, axis = (0,1))\n",
    "        self.update_param(pm[i], str(i)+'_vb', G_bias)\n",
    "        \n",
    "        \n",
    "\n",
    "        g_affine_input = pm[i][str(i)+'_k'].transpose()\n",
    "        G_input_final += np.matmul(G_k, g_affine_input)\n",
    "        g_affine_input = pm[i][str(i)+'_v'].transpose()\n",
    "        G_input_final += np.matmul(G_v, g_affine_input)\n",
    "        g_affine_input = pm[i][str(i)+'_q'].transpose()\n",
    "        G_input_final += np.matmul(G_q, g_affine_input)\n",
    "        self.update_param(pm[i], str(i)+'_k', G_weight_8)\n",
    "        self.update_param(pm[i], str(i)+'_q', G_weight_10)\n",
    "        self.update_param(pm[i], str(i)+'_v', G_weight_9)\n",
    "    G_input_final += new_G_input\n",
    "    \n",
    "    return G_input_final\n",
    "    \n",
    "\n",
    "transformer.backprop_multi_heads_layer=backprop_multi_heads_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self,a='is', b='are'):\n",
    "    a_=self.text_dataset.original_word_to_id[a]\n",
    "    b_=self.text_dataset.original_word_to_id[b]\n",
    "\n",
    "    print(np.dot(self.pm_hiddens[0]['dic'][a_], self.pm_hiddens[0]['dic'][b_])/((sum(self.pm_hiddens[0]['dic'][a_]*self.pm_hiddens[0]['dic'][a_])\\\n",
    "                                                                                **0.5)*(sum(self.pm_hiddens[0]['dic'][b_]*self.pm_hiddens[0]['dic'][b_])**0.5)))\n",
    "\n",
    "def predict_translation(self,sentence_1,t):\n",
    "\n",
    "    result_list=list()\n",
    "    temp_list=list()\n",
    "    sentence_2=self.predict_forward_neuralnet(sentence_1,t)\n",
    "\n",
    "    for i in range(self.batch_size):\n",
    "        temp_list_list=list()\n",
    "        result_list_list=list()\n",
    "        for w in sentence_1.tolist()[i]:\n",
    "            if self.text_dataset.original_unique_words==w:\n",
    "                pass\n",
    "            else:\n",
    "                temp_list_list.append(self.text_dataset.original_id_to_word[w])\n",
    "        temp_list.append(temp_list_list)\n",
    "        \n",
    "        for w in sentence_2.tolist()[i]:\n",
    "            result_list_list.append(self.text_dataset.target_id_to_word[w])\n",
    "        result_list.append(result_list_list)\n",
    "\n",
    "        print(temp_list[i],\"->\",result_list[i])\n",
    "        print(\" \")\n",
    "\n",
    "\n",
    "transformer.test=test\n",
    "transformer.predict_translation=predict_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1c9365cd2b44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malloc_embedding_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malloc_embedding_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_embedding_layer\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mforward_embedding_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop_embedding_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbackprop_embedding_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "def alloc_embedding_layer(self):\n",
    "    input_cnt = len(self.text_dataset.original_id_to_word)\n",
    "    output_cnt = self.word_vector_dimension\n",
    "    a = transformer.making_weight_em(self,input_cnt, output_cnt)\n",
    "    self.G_original_embedding_like=np.zeros_like(a)\n",
    "    return {'dic':a}\n",
    "\n",
    "def forward_embedding_layer(self, x, hconfig, pm):\n",
    "    output=pm['dic'][x,:]\n",
    "    self.input_mask=np.sign(x-len(self.text_dataset.original_id_to_word)+2)+1\n",
    "    self.input_mask[:,0]=0\n",
    "    return output, x\n",
    "\n",
    "def backprop_embedding_layer(self, G_y, hconfig, pm, aux):\n",
    "    if pm is None: return G_y\n",
    "    x= aux\n",
    "    #print(G_y)\n",
    "    for i in range(self.batch_size):\n",
    "        self.G_original_embedding_like[x[i],:]+=G_y[i]\n",
    "    self.update_param(pm, 'dic', self.G_original_embedding_like)\n",
    "    self.G_original_embedding_like=np.zeros_like(self.G_original_embedding_like)\n",
    "    \n",
    "    return 0\n",
    "transformer.alloc_embedding_layer=alloc_embedding_layer\n",
    "transformer.forward_embedding_layer =forward_embedding_layer\n",
    "transformer.backprop_embedding_layer=backprop_embedding_layer\n",
    "\n",
    "def alloc_po_embedding_layer(self):\n",
    "    input_cnt = self.text_dataset.most_length*2\n",
    "    output_cnt = self.word_vector_dimension\n",
    "    a = transformer.making_weight_em(self,input_cnt, output_cnt)\n",
    "    self.G_po_embedding_like=np.zeros_like(a)\n",
    "    return {'dic':a}\n",
    "\n",
    "def forward_po_embedding_layer(self, x, hconfig, pm):\n",
    "    output=np.tile(pm['dic'],self.batch_size).reshape((self.batch_size,\\\n",
    "                                                                            pm['dic'].shape[0],pm['dic'].shape[1]))\n",
    "    #print(output.shape)\n",
    "    return output, x\n",
    "\n",
    "def backprop_po_embedding_layer(self, G_y, hconfig, pm, aux):\n",
    "    if pm is None: return G_y\n",
    "    x= aux\n",
    "    #print(self.G_po_embedding_like.shape,G_.y.shape)\n",
    "    #self.G_po_embedding_like=np.sum(G_y\n",
    "    for i in range(self.batch_size):\n",
    "        self.G_po_embedding_like+=G_y[i]\n",
    "    self.update_param(pm, 'dic', self.G_po_embedding_like)\n",
    "    self.G_po_embedding_like=np.zeros_like(self.G_po_embedding_like)\n",
    "    return 0\n",
    "transformer.alloc_po_embedding_layer=alloc_po_embedding_layer\n",
    "transformer.forward_po_embedding_layer =forward_po_embedding_layer\n",
    "transformer.backprop_po_embedding_layer=backprop_po_embedding_layer\n",
    "\n",
    "def alloc_sep_embedding_layer(self):\n",
    "    input_cnt = 2\n",
    "    output_cnt = self.word_vector_dimension\n",
    "    a = transformer.making_weight_em(self,input_cnt, output_cnt)\n",
    "    self.G_sep_embedding_like=np.zeros_like(a)\n",
    "    return {'dic':a}\n",
    "\n",
    "def forward_sep_embedding_layer(self, x, hconfig, pm):\n",
    "    \n",
    "    output_1=np.tile(pm['dic'][0,:],2*self.text_dataset.most_length*int(self.batch_size//2)).reshape((self.batch_size//2,\\\n",
    "                                                                            2*self.text_dataset.most_length,pm['dic'][0,:].shape[0]))\n",
    "    \n",
    "    output_2=np.tile(pm['dic'][1,:],2*self.text_dataset.most_length*(self.batch_size-int(self.batch_size//2))).reshape((self.batch_size-int(self.batch_size//2),2*self.text_dataset.most_length,pm['dic'][1,:].shape[0]))\n",
    "    output=np.concatenate((output_1,output_2),axis=0)\n",
    "    #print(\"ewr\",output.shape)\n",
    "    return output, x\n",
    "\n",
    "def backprop_sep_embedding_layer(self, G_y, hconfig, pm, aux):\n",
    "    if pm is None: return G_y\n",
    "    x= aux\n",
    "    sum_=0\n",
    "    sum_1=0\n",
    "    #print(self.G_sep_embedding_like[0,:].shape)\n",
    "    \n",
    "    for i in range(self.batch_size):\n",
    "        sum_+=G_y[i,:self.text_dataset.most_length,:]\n",
    "        sum_1+=G_y[i,self.text_dataset.most_length:,:]\n",
    "    #print(sum_.shape)\n",
    "    self.G_sep_embedding_like[0,:]+=np.sum(sum_,axis=0)\n",
    "    self.G_sep_embedding_like[1,:]+=np.sum(sum_1,axis=0)\n",
    "    \n",
    "    self.update_param(pm, 'dic', self.G_sep_embedding_like)\n",
    "    self.G_sep_embedding_like=np.zeros_like(self.G_sep_embedding_like)\n",
    " \n",
    "    return 0\n",
    "transformer.alloc_sep_embedding_layer=alloc_sep_embedding_layer\n",
    "transformer.forward_sep_embedding_layer =forward_sep_embedding_layer\n",
    "transformer.backprop_sep_embedding_layer=backprop_sep_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def forward_mask_layer(self, x):\n",
    "    self.number=np.random.randint(1,2*self.text_dataset.most_length,size=int(2*self.text_dataset.most_length*0.15))\n",
    "    x[:,self.number,:]=0\n",
    "\n",
    "    \n",
    "    return x\n",
    "\n",
    "def backprop_mask_layer(self, x):\n",
    "    return x\n",
    "\n",
    "transformer.forward_mask_layer =forward_mask_layer\n",
    "transformer.backprop_mask_layer=backprop_mask_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
